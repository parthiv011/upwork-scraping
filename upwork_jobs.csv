"title","link","clientSpent","estimatedBudget","jobDescription","paymentVerified"
"CORP - DevOPS required - AWS - CloudOps Network Reward Program","https://www.upwork.com/jobs/CORP-DevOPS-required-span-class-highlight-AWS-span-CloudOps-Network-Reward-Program_~021923289417739262595/?referrer_url_path=/nx/search/jobs/","$8K+
spent","Hourly Expert Est. time: Less than 1 month, Less than 30 hrs/week","Need a DevOps Guy, AWS Cloud expert with good skills. The program related is the CloudOps Network Reward program. Work is quick and short, however, expertise with enterprise-level workload handling is required. Gen AI services is a plus.","Yes"
"Looking for Cloud Architect with GenAI and AWS experience","https://www.upwork.com/jobs/Looking-for-Cloud-Architect-with-GenAI-and-span-class-highlight-AWS-span-experience_~021923218404882489842/?referrer_url_path=/nx/search/jobs/","$100K+
spent","Hourly Expert Est. time: More than 6 months, 30+ hrs/week","Looking for an experienced AWS Cloud Architect with GenAI expertise. Must be AWS certified (Associate or Specialty level). Experience with services like Bedrock, SageMaker, Lambda, and GenAI model integration is required. You'll support solution design, architecture reviews, and proposal input for upcoming projects. Must be AWS certified. No agencies.","Yes"
"Sr. DevOps Engineer Needed for AWS & Cloud Infrastructure Management","https://www.upwork.com/jobs/DevOps-Engineer-Needed-for-span-class-highlight-AWS-span-amp-Cloud-Infrastructure-Management_~021923304439608542808/?referrer_url_path=/nx/search/jobs/","$4K+
spent","Fixed price Expert Est. budget: $50.00","We are seeking an experienced DevOps engineer to manage and optimize our cloud infrastructure across AWS and Cloudflare. This is a long-term engagement requiring expertise in container orchestration, infrastructure automation, and cloud security. Required Experience & Skills 4+ years of experience in DevOps engineering Strong expertise in AWS services (EC2, EKS, ECS, Load Balancers) Proficiency with Infrastructure as Code (Terraform/CloudFormation) Experience with container orchestration (Docker, Kubernetes) Knowledge of Cloudflare services and CDN optimization Strong background in CI/CD implementation Experience with monitoring and alert management Familiarity with IoT infrastructure (AWS Greengrass) Key Responsibilities AWS Infrastructure Management EC2 instance optimization and lifecycle management EKS/ECS cluster setup and maintenance Infrastructure automation using Terraform Resource tagging and cost optimization Cloudflare Integration DNS management and CDN optimization SSL/TLS certificate management WAF configuration and DDoS protection Zero Trust security implementation Container & Application Management Docker image creation and optimization Container security scanning and hardening Multi-container application orchestration Blue-green deployment implementation Database & Cache Management MongoDB Atlas cluster management Redis/Elasticsearch optimization Query performance tuning Cache strategy implementation Monitoring & Support 24/7 infrastructure monitoring Incident response and resolution Performance optimization Log analysis and debugging Project Scope Duration: Long-term engagement (6+ months) Hours per week: 40 hours Project Type: Ongoing development and maintenance Additional Requirements Strong English communication skills Available during standard business hours (UTC-5) Experience with similar-scale infrastructure Ability to provide emergency support when needed Strong documentation practices Budget $8-10/hour, depending on experience level Interested candidates should provide: Detailed experience with similar projects AWS certifications (if any) Previous experience with scalable infrastructure Sample documentation of previous DevOps work Availability for initial technical interview","Yes"
"AWS Big data cleaning & ingestion","https://www.upwork.com/jobs/span-class-highlight-AWS-span-Big-data-cleaning-amp-ingestion_~021923107989533235049/?referrer_url_path=/nx/search/jobs/","$0
spent","Fixed price Expert Est. budget: $150.00","üöÄ Data Pipeline: Clean, Transform & Save Large Property Data from S3 to Database Description: I‚Äôm looking for an experienced data engineer or developer who can help me build an automated solution for processing large property transaction datasets. üîç What I Have: Consistent structured data files stored in an S3 bucket The data includes details related to properties, investors, agents, title companies, and buyers Files are regularly added to the S3 bucket ‚úÖ What I Need: Data Ingestion: Automatically detect or process new files from the S3 bucket. Data Cleaning & Transformation: Parse and transform raw data into structured formats suitable for different views/tables: Investors Agents Title Companies Buyers (Optional: Additional entity-specific breakdowns if needed) Data Loading: Save the cleaned and structured data into my database (PostgreSQL preferred, open to others). Scalability & Reusability: The solution should be efficient, handle large files, and be reusable for future files. üí° Bonus (Nice to Have): Logs or notifications for successful processing Option to trigger manually or via schedule De-duplication logic üìå Tech Stack Preferences: Python (Pandas, SQLAlchemy, etc.) AWS SDK (boto3) PostgreSQL / MySQL Serverless (Lambda) or simple Python script (up to you) If you‚Äôve worked with large-scale data transformation and AWS before, this should be straightforward. Let‚Äôs get this pipeline built so I can focus on analysis, not manual uploads. üôå","Yes"
"DynamoDB Expert Needed for High School Wrestling Tournament System Design","https://www.upwork.com/jobs/DynamoDB-Expert-Needed-for-High-School-Wrestling-Tournament-System-Design_~021923140127097985696/?referrer_url_path=/nx/search/jobs/","$0
spent","Fixed price Expert Est. budget: $3,500.00","We are building a high-performance Wrestling Tournament Management System for high schools and need an experienced AWS DynamoDB data modeler to design an optimal schema. This project requires a strategic data model that supports efficient reads, handles individual event ingestion via API, and accommodates a growing dataset of wrestlers, tournaments, and match records. You‚Äôll design the table structure, define key access patterns, and ensure data consistency for incremental writes.","Yes"
"Configure JWT Authentication with GeoServer + MongoDB Integration in Kubernetes","https://www.upwork.com/jobs/Configure-JWT-Authentication-with-GeoServer-MongoDB-Integration-Kubernetes_~021923173548608853287/?referrer_url_path=/nx/search/jobs/","$0
spent","Hourly: $15.00 - $30.00 Expert Est. time: 1 to 3 months, Less than 30 hrs/week","Description: We're seeking a skilled DevOps or backend engineer with hands-on experience configuring secure authentication and optimizing map service performance in Kubernetes environments. This project involves integrating a GeoServer instance with JWT-based authentication (via AWS Cognito), connecting it to a MongoDB data source, and enabling caching of vector tiles in cloud storage (S3). The goal is to improve the performance and security of a map-heavy SaaS application. Key Responsibilities: -Configure JWT authentication in GeoServer, ensuring compatibility with existing token structure. -Set up GeoServer to connect to MongoDB collections as a data source. -Implement caching of vector tiles to AWS S3 to enhance map rendering performance. -Secure all service connections and ensure access controls are properly enforced. Technical Stack: -GeoServer (deployed in Kubernetes) -JWT (JSON Web Tokens) via AWS Cognito or equivalent -MongoDB for spatial data -AWS S3 for tile caching -Kubernetes (on a cloud VPS provider, not EKS) Preferred Qualifications: -Experience configuring GeoServer with custom JWT alidation -Strong understanding of Kubernetes deployments -Prior work integrating MongoDB with map services -Familiarity with vector tile formats and caching mechanisms -Ability to work independently and communicate technical details clearly","Yes"
"URGENT Need Expert with AWS Server - Site is Down","https://www.upwork.com/jobs/URGENT-Need-Expert-with-span-class-highlight-AWS-span-Server-Site-Down_~021923171094153926505/?referrer_url_path=/nx/search/jobs/","$30K+
spent","Hourly: $35.00 - $75.00 Expert Est. time: Less than 1 month, Less than 30 hrs/week","The site is down, current developer cannot solve the issue. Keeps timing out, multiple restarts today, but bot activity apparently keeps overloading.","Yes"
"Automation with Python using boto3 in AWS","https://www.upwork.com/jobs/Automation-with-Python-using-boto3-span-class-highlight-AWS-span_~021923064434385884455/?referrer_url_path=/nx/search/jobs/","$100+
spent","Hourly: $15.00 - $25.00 Expert Est. time: Less than 1 month, Less than 30 hrs/week","I am starting my journey on python automation in AWS. Need some assistance on an ongoing basis where I get stuck or need help troubleshooting","Yes"
"Next.js, AWS Cognito Authentication","https://www.upwork.com/jobs/Next-span-class-highlight-AWS-span-Cognito-Authentication_~021923154831749551399/?referrer_url_path=/nx/search/jobs/","$2K+
spent","Fixed price Expert Est. budget: $20.00","Looking for a full stack developer who can implement authentication for Next.js project using AWS Cognito. If you have a strong experience and can do this in a few hours, please apply. More tasks will arise if the result is great.","Yes"
"AWS Solutions Architect ‚Äì AI/ML Focus- US Only- No Agencies","https://www.upwork.com/jobs/span-class-highlight-AWS-span-Solutions-Architect-Focus-Only-Agencies_~021922969449560233794/?referrer_url_path=/nx/search/jobs/","$1M+
spent","Hourly Expert Est. time: 3 to 6 months, 30+ hrs/week","Job Objective Experienced AWS Solutions Architect with a strong focus on Data Engineering, Generative AI, and Machine Learning. The ideal candidate will help design and deliver scalable cloud solutions for clients, combining AWS-native services with AI/ML capabilities. This role requires a mix of technical architecture, presales consulting, and delivery support for innovative, data-driven solutions. Requirements ‚Ä¢ Extensive experience designing, building, and operating solutions on AWS. ‚Ä¢ Strong expertise in AI/ML architectures, data engineering, and cloud-native design. ‚Ä¢ Proven experience with AWS AI/ML services like SageMaker, AWS Bedrock, and integrations with OpenAI. ‚Ä¢ Hands-on with AWS data services: Glue, Kinesis, Redshift, Athena, and data lakes. ‚Ä¢ Ability to design containerized AI/ML solutions using Docker, ECS, EKS, or Fargate. ‚Ä¢ Proficiency with Terraform or CloudFormation for infrastructure as code. ‚Ä¢ Solid grasp of AWS security and compliance best practices. ‚Ä¢ Strong client-facing and presales experience‚Äîable to translate business needs into scalable technical solutions. ‚Ä¢ Preferred: AWS Certified Solutions Architect (Associate or Professional)","Yes"
"Experienced API Developer Needed for Healthcare Project (Python, GCP, Apigee, Terraform)","https://www.upwork.com/jobs/Experienced-API-Developer-Needed-for-Healthcare-Project-Python-GCP-Apigee-Terraform_~021922886067294592547/?referrer_url_path=/nx/search/jobs/","$0
spent","Fixed price Expert Est. budget: $280.00","Title: Experienced API Developer Needed for Healthcare Project (Python, GCP, Apigee, Terraform) Description: We are seeking a skilled API developer to build robust and secure APIs from the ground up for a healthcare client. The project involves integrating with Google Cloud Platform (GCP), Apigee API Gateway, and utilizing Terraform for infrastructure as code. Project Details: Duration: 2 to 4 hours (to be finalized during the demo) Budget: ‚Çπ24,000 for 2 hours Responsibilities: Design and develop RESTful APIs tailored for healthcare applications. Deploy and manage APIs on GCP using Apigee. Implement infrastructure as code using Terraform for scalable deployments. Ensure compliance with healthcare data security standards. Requirements: Proven experience in API development using Python. Hands-on experience with GCP services and Apigee API Gateway. Proficiency in Terraform for infrastructure provisioning. Familiarity with healthcare industry standards and compliance (e.g., HIPAA). Strong problem-solving skills and attention to detail. Preferred Qualifications: Experience with CI/CD pipelines and DevOps practices. Knowledge of additional cloud platforms (e.g., AWS, Azure) is a plus.","Yes"
"Amazon Titan AI Integration Specialist for Video Platform","https://www.upwork.com/jobs/span-class-highlight-Amazon-span-Titan-Integration-Specialist-for-Video-Platform_~021922797392384089498/?referrer_url_path=/nx/search/jobs/","$90K+
spent","Hourly: $22.00 - $50.00 Expert Est. time: More than 6 months, 30+ hrs/week","We are seeking a skilled professional to integrate Amazon Titan AI with our AI video platform. The ideal candidate will have experience in leveraging AI technologies to enhance video processing and analytics. You will be responsible for developing seamless connectivity between the systems and ensuring optimal performance and scalability. Strong problem-solving skills and a good understanding of both Amazon services and video platforms are essential. If you thrive in a dynamic environment and have a passion for AI innovation, we want to hear from you! **Relevant Skills:** - Amazon Web Services (AWS) - AI and Machine Learning - Video Processing Technologies - API Integration - Software Development - Problem-Solving Skills","Yes"
"Full Stack Engineer","https://www.upwork.com/jobs/Full-Stack-Engineer_~021922797731256244653/?referrer_url_path=/nx/search/jobs/","$1K+
spent","Hourly: $25.00 - $30.00 Expert Est. time: 1 to 3 months, 30+ hrs/week","Full Stack Engineer Location: [Remote] Employment Type: [Contract] Company Website: https://spanhead.tech About Us Spanhead is a cutting-edge software consulting company specializing in innovative technology solutions. We pride ourselves on delivering high-quality applications using modern development frameworks and cloud architectures. We are looking for a highly skilled Full Stack Engineer with expertise in both frontend and mobile development to join our growing team. Position Overview As a Full Stack Engineer, you will be responsible for designing, developing, and deploying scalable web and mobile applications. You will play a critical role in both frontend and backend development, with a strong emphasis on Next.js/React expertise for web applications and React for mobile apps. Additionally, you will work on DevOps practices with AWS cloud, ensuring efficient deployment, monitoring, and optimization of applications. Responsibilities Develop and maintain full-stack applications, focusing on frontend engineering using Next.js/React. Design, develop, and optimize mobile applications using React Native or other cross-platform mobile frameworks. Collaborate with designers, product managers, and backend developers to implement intuitive user experiences across web and mobile platforms. Ensure seamless integration between frontend, backend, and cloud services. Implement DevOps workflows for automated deployments, CI/CD pipelines, and cloud infrastructure management in AWS. Troubleshoot, debug, and resolve technical issues across the stack. Stay up to date with industry trends and best practices in modern web, mobile development, and DevOps. Requirements Minimum 3 years of experience as a Senior Frontend Engineer, specializing in Next.js/React (must-have). Strong proficiency in React and iOS native development Experience with backend development using Node.js, Python, or similar technologies. Expertise in DevOps practices with AWS, including deployment, monitoring, and scaling applications. Familiarity with database management (SQL, NoSQL) and server-side technologies. Experience with version control systems (Git) and CI/CD pipelines. Strong problem-solving skills and ability to work in an agile development environment. Excellent communication and collaboration skills. Preferred Qualifications Experience with microservices architecture and containerization (Docker, Kubernetes). Knowledge of serverless frameworks and cloud-native development. Familiarity with testing frameworks (such as Playwright.dev) for frontend and mobile applications. Strong understanding of performance optimization techniques for web and mobile applications. Benefits Competitive salary and performance-based bonuses. Flexible work arrangements (remote/hybrid). Opportunities for professional growth and continuous learning. Engaging and innovative company culture with exciting projects.","Yes"
"Senior Full Stack Engineer","https://www.upwork.com/jobs/Senior-Full-Stack-Engineer_~021922727323484068269/?referrer_url_path=/nx/search/jobs/","$0
spent","Hourly: $22.00 - $70.00 Expert Est. time: 3 to 6 months, Less than 30 hrs/week","About Us We‚Äôre Coretext, an early-stage startup equipping enterprise automations with AI. Companies will no longer have to manually trigger their workflows with Coretext. We‚Äôre moving fast, learning even faster, and looking for sharp builders who want to help shape the product and tech from the ground up. What We Need We‚Äôre looking for a Senior Full Stack Engineer who can jump in, own projects, and ship real impact immediately. You'll work closely with the founders to bring features to life, solve tricky problems, and lay the foundation for our next phase. What You'll Do Build and ship features across the Coretext console (front-end, back-end, infra ‚Äî whatever it takes). Code with AI and organize your workflows with AI tools Make technical decisions with long-term impact. Move fast: short feedback loops, rapid iteration. Work directly with users and the founding team to prioritize and deliver. Help set up lightweight, sustainable engineering practices. Skills We're Looking For 5+ years full stack experience ‚Äî comfortable with Javascript, typescript, python Postgres, React, Node.js, AWS. 1-2 years building AI agents or working with AI systems. Be curious about the latest tech Strong fundamentals: clean code, scalable systems, problem-solving mindset. Self-starter: you can take a feature from ""idea"" to ""in production"" without a lot of hand-holding. Flexible and pragmatic ‚Äî you know when to be scrappy and when to build for scale. Clear communicator, async-friendly. Nice to Haves Experience working with enterprise technology Understanding of security architecture Familiarity with GraphQL, Docker, CI/CD pipelines, serverless, etc Interest in product thinking ‚Äî not just ""what"" but ""why."" Experience taking product release & deployment ownership Contract Details Start: Mid-May 2025 Hours: ~20‚Äì40 /week Rate: Competitive hourly or project-based Duration: 3 months with possible extension Potential to transition to full-time if there‚Äôs mutual fit. Why Work With Us? Competitive pay. True ownership over what you build. Low bureaucracy, high autonomy. Work closely with a small, passionate team. Opportunity to influence product, culture, and tech from day one.","Yes"
"AWS Data Engineer ‚Äì Build Scalable Data Pipelines & Architecture for Business Intelligence Portal","https://www.upwork.com/jobs/span-class-highlight-AWS-span-Data-Engineer-Build-Scalable-Data-Pipelines-amp-Architecture-for-Business-Intelligence-Portal_~021922648323136562355/?referrer_url_path=/nx/search/jobs/","$40K+
spent","Hourly: $30.00 - $30.00 Expert Est. time: 3 to 6 months, Less than 30 hrs/week","Description: We‚Äôre looking for an experienced AWS Data Engineer to help design and implement a modern data portal that brings together siloed business data for reporting, modeling, and visualization. This role will involve building ETL/ELT pipelines, establishing scalable cloud data infrastructure, and working with a variety of data sources‚Äîincluding CRM, ERP, marketing platforms, and legacy systems. Project Goals: - Create a centralized data portal on AWS for analytics and BI - Build efficient pipelines for ingesting data from multiple business systems - Model data for reporting and visualization layers - Lay the groundwork for scalable, cloud-native data architecture What You‚Äôll Do: - Design and implement ETL/ELT pipelines using AWS-native tools (Glue, Lambda, etc.) - Work with S3 for data lake storage and RDS (MySQL) for structured reporting layers - Build data workflows that bring together CRM, ERP, marketing, and legacy data - Optimize data models for querying, analytics, and BI tools - Set up EC2 instances or orchestration tools (e.g., Airflow or dbt) as needed - Ensure data reliability, scalability, and performance across systems Must-Have Skills: - Proven experience building data pipelines (ETL/ELT) - Strong AWS expertise (S3, RDS, EC2, Glue, Lambda, Athena) - Solid understanding of modern data architecture (lake vs. warehouse) - Data modeling experience for analytics and BI - Ability to integrate data from CRM, ERP, and marketing platforms - Familiarity with SQL and Python for transformation and orchestration Nice-to-Have: - Experience with tools like dbt, Airflow, Redshift, or QuickSight - Knowledge of legacy data systems and extraction strategies - Familiarity with BI tools (Power BI, Tableau, etc.) Project Type: Short-term project with opportunity for ongoing work as the portal evolves. Estimated Duration: 1‚Äì3 months (initial build), with potential for continued development and maintenance work.","Yes"
"DevOps Expert","https://www.upwork.com/jobs/DevOps-Expert_~021922713348650342829/?referrer_url_path=/nx/search/jobs/","$400K+
spent","Hourly: $25.00 - $40.00 Expert Est. time: More than 6 months, 30+ hrs/week","We are looking for 1 or more engineers with the below experience. If you feel you are a good fit, please send us a note explaining the most comprehensive and impressive project that you have worked on. Sending a AI generated response is a waste of your time and mine, if you have not worked on a similar project. We can work out your experiece in about 30 seconds :) Please be also prepared to present yourself at an interview via video confierence with VIDEO ON. 1. Terraform (Infrastructure as Code) Expertise 2. Kubernetes Deployment and Management 3. Ingress Controller Configuration (ALB and NGINX) 4. Jenkins Setup and Configuration 5. Jenkins Pipeline Scripting (Bash, Python, Groovy) 6. AWS API Gateway Configuration and Management 7. AWS CloudFront and WAF (Web Application Firewall) ‚Äì Policies and Rules Management 8. S3 Configuration ‚Äì Permissions, Access Policies, and Bucket Policies 9. COMF Knowledge 10. Frontend Build and Deployment knowledge (Node.js and React.js Applications) 11. Database Management - PostgreSQL, database design, ORM tools 12. Real-time Communication using Socket.IO: 13. Message Queues / Event-driven Architecture, Apache Kafka: If you feel you have the above experience, please send your details. In your response, kindly detail your experience in relation to the above. Thank you. Rana Peries","Yes"
"AWS DevOps Engineer ‚Äì Containerized Backend Migration","https://www.upwork.com/jobs/span-class-highlight-AWS-span-DevOps-Engineer-Containerized-Backend-Migration_~021922700949820399002/?referrer_url_path=/nx/search/jobs/","$0
spent","Fixed price Expert Est. budget: $1,200.00","IMPORTANT: This opportunity is only open to Spanish‚Äêspeaking candidates. We‚Äôre looking for a DevOps Engineer to migrate our container-based backend (Django, Postgres with geospatial extensions, Redis, RabbitMQ, Nginx, etc.) to AWS infrastructure. We currently manage separate staging and production environments with Docker Compose and Makefiles, and we need a scalable, secure transition. Responsibilities: - Design and implement the AWS architecture (containerization, orchestration, data storage). - Adapt and optimize existing services (Django, Celery, reverse proxy, etc.) for cloud environments. - Manage secrets and multi-environment configurations (staging and production) using secure AWS practices (IAM, Secrets Manager, etc.). - Ensure system security and availability, as well as cost optimization in production. - Desirable: Create and maintain CI/CD pipelines that automate deployment, integrate JMeter tests, and monitor the application. Requirements: - Strong AWS experience (EC2, ECS/EKS, RDS, S3, IAM, etc.) with a DevOps mindset. - Advanced knowledge of Docker, Docker Compose, and microservices. - Experience with infrastructure as code (Terraform, CloudFormation, or similar). - Familiarity with environments using RabbitMQ, Redis, and asynchronous task orchestration (Celery). - Ability to diagnose issues in distributed systems and optimize performance. Join us and take our infrastructure to the next level on AWS!","Yes"
"Senior/Tech Lead Data Engineer","https://www.upwork.com/jobs/Senior-Tech-Lead-Data-Engineer_~021922640449366795443/?referrer_url_path=/nx/search/jobs/","$200K+
spent","Hourly: $40.00 - $45.00 Expert Est. time: More than 6 months, 30+ hrs/week","As a Senior/Tech Lead Data Engineer, you will play a pivotal role in designing, implementing, and optimizing data platforms. Your primary responsibilities will revolve around data modeling, ETL development, and platform optimization, leveraging technologies such as EMR/Glue, Airflow, Spark, using Python and various cloud-based solutions. Key Responsibilities: Design, develop, and maintain ETL pipelines for ingesting and transforming data from diverse sources. Collaborate with cross-functional teams to ensure seamless deployment and integration of data solutions. Lead efforts in performance tuning and query optimization to enhance data processing efficiency. Provide expertise in data modeling and database design to ensure scalability and reliability of data platforms. Contribute to the development of best practices and standards for data engineering processes. Stay updated on emerging technologies and trends in the data engineering landscape. Required Skills and Qualifications: Bachelor's Degree in Computer Science or related field. Minimum of 5 years of experience in tech lead data engineering or architecture roles. Proficiency in Python and PySpark for ETL development and data processing. AWS CLOUD at least 2 years Extensive experience with cloud-based data platforms, particularly EMR. Must have knowledge with Spark. Excellent problem-solving skills and ability to work effectively in a collaborative team environment. Leadership experience, with a proven track record of leading data engineering teams.","Yes"
"API Developer Needed for Healthcare Project (Python, GCP, Apigee, Terraform) 5+ years experience","https://www.upwork.com/jobs/API-Developer-Needed-for-Healthcare-Project-Python-GCP-Apigee-Terraform-years-experience_~021922613289955191414/?referrer_url_path=/nx/search/jobs/","$0
spent","Fixed price Expert Est. budget: $258.00","Title: Experienced API Developer Needed for Healthcare Project (Python, GCP, Apigee, Terraform) Description: We are seeking a skilled API developer to build robust and secure APIs from the ground up for a healthcare client. The project involves integrating with Google Cloud Platform (GCP), Apigee API Gateway, and utilizing Terraform for infrastructure as code. Project Details: Duration: 2 to 4 hours (to be finalized during the demo) Budget: ‚Çπ24,000 for 2 hours Responsibilities: Design and develop RESTful APIs tailored for healthcare applications. Deploy and manage APIs on GCP using Apigee. Implement infrastructure as code using Terraform for scalable deployments. Ensure compliance with healthcare data security standards. Requirements: Proven experience in API development using Python. Hands-on experience with GCP services and Apigee API Gateway. Proficiency in Terraform for infrastructure provisioning. Familiarity with healthcare industry standards and compliance (e.g., HIPAA). Strong problem-solving skills and attention to detail. Preferred Qualifications: Experience with CI/CD pipelines and DevOps practices. Knowledge of additional cloud platforms (e.g., AWS, Azure) is a plus.","Yes"
"Cloud & DevOps Expert","https://www.upwork.com/jobs/Cloud-DevOps-Expert_~021922611410417624243/?referrer_url_path=/nx/search/jobs/","$0
spent","Hourly Expert Est. time: 1 to 3 months, Less than 30 hrs/week","We are looking for a highly skilled Cloud DevOps Engineer to evaluate and improve our existing AWS infrastructure. This role demands a hands-on professional who can independently review our current architecture, identify inefficiencies or gaps, and provide actionable suggestions to enhance performance, security, and cost optimization. The engineer should be able to recommend and implement improvements while aligning with our team‚Äôs workflows and priorities. The ideal candidate should have deep expertise in AWS services including EC2, S3, RDS, IAM, VPC, Load Balancers, AWS security tools like WAF, and Route 53 etc. Strong experience with monitoring tools such as Amazon CloudWatch and Datadog is essential. You should be confident giving suggestions for our Python-based application deployed on AWS, with familiarity in using Gunicorn. We expect you to bring and suggest DevOps best practices. Proficiency in shell scripting and Python for automation is a must. Let‚Äôs connect one-on-one to discuss this role and the requirements in more detail.","Yes"